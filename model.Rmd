---
title: "Model"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load("lme4")
```

```{r}
library(dplyr)
eng<-read.csv("eng.csv")
#eng
spa<-read.csv("spa.csv")
#spa
```

```{r}
#44 items labeled as cognate/noncognate.
eng1<-eng%>%filter(!word %in% c('bed','tree', 'pencil', 'house', 'whistle','scissors','saw','toothbrush','pretzel','bench','globe','cactus','escalator','pelican',
                              'stethoscope','palette'))
#eng1

```

```{r}
#code： cognate=1, noncognate=0.
eng1$cognate[eng1$word %in% c('comb','broom','octopus','mushroom','hanger','wheelchair','snail','seahorse','wreath',
'beaver','stilts','acorn','knocker','muzzle','funnel','noose','latch','scroll','tongs','yoke','trellis','protractor')] <- 0
eng1$cognate[!eng1$word %in% c('comb','broom','octopus','mushroom','hanger','wheelchair','snail','seahorse','wreath',
'beaver','stilts','acorn','knocker','muzzle','funnel','noose','latch','scroll','tongs','yoke','trellis','protractor')] <- 1
eng1
#write.csv(eng1,"eng1.csv",row.names=FALSE)
```

```{r}
#44 items labeled as cognate/noncognate.
spa1<-spa%>%filter(!word %in% c('bed','tree', 'pencil', 'house', 'whistle','scissors','saw','toothbrush','pretzel','bench','globe','cactus','escalator','pelican',
                              'stethoscope','palette'))
#spa1
```

```{r}
#code： cognate=1, noncognate=0.
spa1$cognate[spa1$word %in% c('comb','broom','octopus','mushroom','hanger','wheelchair','snail','seahorse','wreath',
'beaver','stilts','acorn','knocker','muzzle','funnel','noose','latch','scroll','tongs','yoke','trellis','protractor')] <- 0
spa1$cognate[!spa1$word %in% c('comb','broom','octopus','mushroom','hanger','wheelchair','snail','seahorse','wreath',
'beaver','stilts','acorn','knocker','muzzle','funnel','noose','latch','scroll','tongs','yoke','trellis','protractor')] <- 1
spa1
#write.csv(spa1,"spa1.csv",row.names=FALSE)
```

```{r}
#logistic regression model for english data:
M1<-glm(accuracy~cognate, family=binomial(link="logit"),data=eng1)
summary(M1)
```

invlogit(-0.56+0.33*1)=0.44
invlogit(-0.56)=0.36


```{r}
#logistic regression model for spanish data:
M2<-glm(accuracy~cognate, family=binomial(link="logit"),data=spa1)
summary(M2)
```

invlogit(-1.23+0.69*1)=0.37
invlogit(-1.23)=0.23


## Add random variable into logistic model 
```{r}
M3 <- glmer(accuracy~1+(1|cognate),data=eng1,family=binomial(link="logit"))
print(M3)
```

```{r}
M4 <- glmer(accuracy~1+(1|cognate),data=spa1,family=binomial(link="logit"))
print(M4)
```

##add difficulty into the dataset
```{r}
spa2 <- read.csv("spa2.csv",header = T)
eng2 <- read.csv("eng2.csv",header = T)
eng2$diff <- as.numeric(eng2$diff)
spa2$diff <- as.numeric(spa2$diff)
```

## refit the model 
```{r}
M5 <- glmer(accuracy~cognate+(1|subject)+diff,data=eng2,family=binomial(link="logit"))
print(M5)

library(boot)
inv.logit(2.1693)
inv.logit(0.7015)
inv.logit(-0.8832)
```

Interpret:
Intercept: For non-cognate words with same difficulty level, the possibility of answer the words accurately is 89.76%.
Cognate: Words with same difficulty level, cognate worlds tend to have 66.85% higher possibility to be correctly answered.
diff: For non-Cognates word, it has 29.25% higher possibility to be correctly answered as the difficulty increasing. 

```{r}
M6 <- glmer(accuracy~cognate+(1|subject)+diff,data=spa2,family=binomial(link="logit"))
print(M6)
inv.logit(1.055)
inv.logit(1.1634)
inv.logit(-0.8374)
```

Interpret:
Intercept: For non-cognate words with same difficulty level, the possibility of answer the words accurately is 74.17%.
Cognate: Words with same difficulty level, cognate worlds tend to have 76.195% higher possibility to be correctly answered.
diff: For non-Cognates word, it has 30.2% higher possibility to be correctly answered as the difficulty increasing. 

